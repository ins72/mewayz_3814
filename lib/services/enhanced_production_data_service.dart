import 'dart:async';
import 'dart:io';

import 'package:supabase_flutter/supabase_flutter.dart';

import '../core/app_export.dart';

/// Enhanced production data service with enterprise-grade optimizations
class EnhancedProductionDataService {
  static final EnhancedProductionDataService _instance = EnhancedProductionDataService._internal();
  factory EnhancedProductionDataService() => _instance;
  EnhancedProductionDataService._internal();

  late final SupabaseClient _client;
  bool _isInitialized = false;
  String? _currentWorkspaceId;
  
  // Advanced caching layers
  final Map<String, dynamic> _l1Cache = {}; // Memory cache
  final Map<String, DateTime> _cacheExpiry = {};
  final Map<String, int> _cacheHitCounts = {};
  
  // Performance monitoring
  final Map<String, List<double>> _performanceMetrics = {};
  Timer? _performanceMonitorTimer;
  Timer? _cacheOptimizationTimer;
  Timer? _healthCheckTimer;
  
  // Real-time subscriptions with connection resilience
  final Map<String, RealtimeChannel> _realtimeChannels = {};
  final Map<String, StreamController<Map<String, dynamic>>> _dataStreams = {};
  
  // Background sync queue
  final List<Map<String, dynamic>> _pendingOperations = [];
  bool _isSyncing = false;
  
  /// Initialize the enhanced production data service
  Future<void> initialize() async {
    if (_isInitialized) return;
    
    try {
      _client = await SupabaseService.instance.client;
      
      // Initialize workspace context
      await _initializeWorkspaceContext();
      
      // Start performance monitoring
      _startAdvancedMonitoring();
      
      // Initialize intelligent caching
      _initializeIntelligentCaching();
      
      // Setup connection resilience
      _setupConnectionResilience();
      
      _isInitialized = true;
      debugPrint('‚úÖ Enhanced Production Data Service initialized');
      
      // Track initialization
      await _trackSystemMetric('enhanced_data_service_initialized', 1);
    } catch (e) {
      debugPrint('‚ùå Enhanced Production Data Service initialization failed: $e');
      rethrow;
    }
  }

  Future<void> _ensureInitialized() async {
    if (!_isInitialized) {
      await initialize();
    }
  }

  /// Initialize workspace context with caching
  Future<void> _initializeWorkspaceContext() async {
    try {
      _currentWorkspaceId = await _getOptimizedData<String>(
        'current_workspace_id',
        () async {
          final userId = _client.auth.currentUser?.id;
          if (userId == null) return '';
          
          final response = await _client
              .from('workspace_members')
              .select('workspace_id')
              .eq('user_id', userId)
              .eq('is_active', true)
              .order('joined_at', ascending: false)
              .limit(1);
          
          return response.isNotEmpty ? response.first['workspace_id'] as String : '';
        },
        ttl: const Duration(minutes: 30),
      );
    } catch (e) {
      debugPrint('Error initializing workspace context: $e');
    }
  }

  /// Start advanced performance monitoring
  void _startAdvancedMonitoring() {
    _performanceMonitorTimer = Timer.periodic(const Duration(minutes: 1), (_) {
      _collectAdvancedMetrics();
    });
    
    _healthCheckTimer = Timer.periodic(const Duration(minutes: 5), (_) {
      _performComprehensiveHealthCheck();
    });
  }

  /// Initialize intelligent caching system
  void _initializeIntelligentCaching() {
    _cacheOptimizationTimer = Timer.periodic(const Duration(minutes: 10), (_) {
      _optimizeCacheStrategy();
    });
  }

  /// Setup connection resilience for real-time features
  void _setupConnectionResilience() {
    // Monitor connection state and auto-reconnect
    Timer.periodic(const Duration(minutes: 2), (_) {
      _checkAndRestoreConnections();
    });
  }

  /// Enhanced data fetching with multi-layer caching and performance optimization
  Future<T?> _getOptimizedData<T>(
    String cacheKey,
    Future<T> Function() dataProvider, {
    Duration? ttl,
    bool forceRefresh = false,
    bool trackPerformance = true,
  }) async {
    final stopwatch = Stopwatch()..start();
    
    try {
      // Check L1 cache first
      if (!forceRefresh && _l1Cache.containsKey(cacheKey)) {
        final expiry = _cacheExpiry[cacheKey];
        if (expiry != null && DateTime.now().isBefore(expiry)) {
          _cacheHitCounts[cacheKey] = (_cacheHitCounts[cacheKey] ?? 0) + 1;
          if (trackPerformance) {
            stopwatch.stop();
            _recordMetric('cache_hit_time_ms', stopwatch.elapsedMilliseconds.toDouble());
            await _trackCachePerformance(cacheKey, true, stopwatch.elapsedMilliseconds);
          }
          return _l1Cache[cacheKey] as T;
        }
      }
      
      // Try database cache
      final dbCacheResult = await _getDatabaseCache(cacheKey);
      if (dbCacheResult != null && !forceRefresh) {
        _l1Cache[cacheKey] = dbCacheResult;
        _cacheExpiry[cacheKey] = DateTime.now().add(ttl ?? const Duration(minutes: 15));
        _cacheHitCounts[cacheKey] = (_cacheHitCounts[cacheKey] ?? 0) + 1;
        
        if (trackPerformance) {
          stopwatch.stop();
          _recordMetric('db_cache_hit_time_ms', stopwatch.elapsedMilliseconds.toDouble());
          await _trackCachePerformance(cacheKey, true, stopwatch.elapsedMilliseconds);
        }
        return dbCacheResult as T;
      }
      
      // Fetch fresh data
      final data = await dataProvider();
      
      if (data != null) {
        // Store in L1 cache
        _l1Cache[cacheKey] = data;
        _cacheExpiry[cacheKey] = DateTime.now().add(ttl ?? const Duration(minutes: 15));
        _cacheHitCounts[cacheKey] = 0;
        
        // Store in database cache asynchronously
        _storeDatabaseCache(cacheKey, data, ttl ?? const Duration(minutes: 15));
      }
      
      if (trackPerformance) {
        stopwatch.stop();
        _recordMetric('data_fetch_time_ms', stopwatch.elapsedMilliseconds.toDouble());
        await _trackCachePerformance(cacheKey, false, stopwatch.elapsedMilliseconds);
      }
      
      return data;
    } catch (e) {
      if (trackPerformance) {
        stopwatch.stop();
        _recordMetric('data_fetch_error_time_ms', stopwatch.elapsedMilliseconds.toDouble());
      }
      debugPrint('Optimized data fetch error: $e');
      rethrow;
    }
  }

  /// Get data from database cache with intelligent analytics
  Future<dynamic> _getDatabaseCache(String cacheKey) async {
    try {
      final result = await _client.rpc('get_cached_result', params: {
        'cache_key_param': cacheKey,
        'workspace_uuid': _currentWorkspaceId,
      });
      
      if (result != null) {
        await _trackSystemMetric('db_cache_hit', 1);
        return result;
      }
      return null;
    } catch (e) {
      await _trackSystemMetric('db_cache_error', 1);
      return null;
    }
  }

  /// Store data in database cache with performance tracking
  Future<void> _storeDatabaseCache(String cacheKey, dynamic data, Duration ttl) async {
    try {
      final jsonData = json.encode(data);
      await _client.rpc('set_cache_result', params: {
        'cache_key_param': cacheKey,
        'result_data_param': json.decode(jsonData),
        'workspace_uuid': _currentWorkspaceId,
        'ttl_seconds': ttl.inSeconds,
      });
      
      await _trackSystemMetric('db_cache_store', 1);
    } catch (e) {
      await _trackSystemMetric('db_cache_store_error', 1);
    }
  }

  /// Enhanced analytics data with predictive insights
  Future<Map<String, dynamic>> getEnhancedAnalyticsData({
    bool includePredictor = true,
    bool includeTrends = true,
    Duration? cacheTTL,
  }) async {
    try {
      await _ensureInitialized();
      
      return await _getOptimizedData<Map<String, dynamic>>(
        'enhanced_analytics_${_currentWorkspaceId}_$includePredictor',
        () async {
          final baseAnalytics = await _getBaseAnalytics();
          
          if (includePredictor) {
            final predictions = await _getPredictiveAnalytics();
            baseAnalytics['predictions'] = predictions;
          }
          
          if (includeTrends) {
            final trends = await _getTrendAnalytics();
            baseAnalytics['trends'] = trends;
          }
          
          // Add real-time performance metrics
          baseAnalytics['performance'] = await _getRealtimePerformanceMetrics();
          
          return baseAnalytics;
        },
        ttl: cacheTTL ?? const Duration(minutes: 5),
      ) ?? {};
    } catch (e) {
      ErrorHandler.handleError('Failed to get enhanced analytics data: $e');
      return {};
    }
  }

  /// Get base analytics with optimization
  Future<Map<String, dynamic>> _getBaseAnalytics() async {
    if (_currentWorkspaceId == null) return {};
    
    final response = await _client.rpc('get_workspace_dashboard_analytics', 
      params: {'workspace_uuid': _currentWorkspaceId});
    
    return response ?? {};
  }

  /// Get predictive analytics using ML-style analysis
  Future<Map<String, dynamic>> _getPredictiveAnalytics() async {
    try {
      final response = await _client.rpc('analyze_predictive_scaling', params: {
        'workspace_uuid': _currentWorkspaceId,
        'forecast_hours': 24,
      });
      
      return response ?? {};
    } catch (e) {
      debugPrint('Predictive analytics error: $e');
      return {};
    }
  }

  /// Get trend analytics
  Future<Map<String, dynamic>> _getTrendAnalytics() async {
    try {
      // Analyze trends over the past week
      final response = await _client
          .from('workspace_metrics')
          .select('*')
          .eq('workspace_id', _currentWorkspaceId ?? '')
          .gte('metric_date', DateTime.now().subtract(const Duration(days: 7)).toIso8601String().substring(0, 10))
          .order('metric_date', ascending: false);
      
      final metrics = List<Map<String, dynamic>>.from(response);
      
      // Calculate trends
      final trends = <String, dynamic>{};
      final groupedMetrics = <String, List<Map<String, dynamic>>>{};
      
      for (final metric in metrics) {
        final metricName = metric['metric_name'] as String;
        groupedMetrics.putIfAbsent(metricName, () => []).add(metric);
      }
      
      groupedMetrics.forEach((metricName, metricList) {
        if (metricList.length > 1) {
          final values = metricList.map((m) => (m['metric_value'] as num).toDouble()).toList();
          final trend = _calculateTrend(values);
          trends[metricName] = {
            'trend_direction': trend > 0.1 ? 'increasing' : trend < -0.1 ? 'decreasing' : 'stable',
            'trend_percentage': trend * 100,
            'current_value': values.first,
            'previous_value': values.last,
          };
        }
      });
      
      return trends;
    } catch (e) {
      debugPrint('Trend analytics error: $e');
      return {};
    }
  }

  /// Calculate trend from values
  double _calculateTrend(List<double> values) {
    if (values.length < 2) return 0.0;
    
    final firstValue = values.last;
    final lastValue = values.first;
    
    if (firstValue == 0) return 0.0;
    
    return (lastValue - firstValue) / firstValue;
  }

  /// Get real-time performance metrics
  Future<Map<String, dynamic>> _getRealtimePerformanceMetrics() async {
    try {
      final response = await _client.rpc('monitor_real_time_performance');
      return response ?? {};
    } catch (e) {
      debugPrint('Real-time performance metrics error: $e');
      return {};
    }
  }

  /// Enhanced real-time subscriptions with intelligent reconnection
  Future<Stream<Map<String, dynamic>>> subscribeToEnhancedRealTimeData({
    required String table,
    Map<String, dynamic>? filters,
    bool enablePredictions = false,
  }) async {
    try {
      await _ensureInitialized();
      
      final streamKey = '${table}_${_currentWorkspaceId}';
      
      // Return existing stream if available
      if (_dataStreams.containsKey(streamKey)) {
        return _dataStreams[streamKey]!.stream;
      }
      
      // Create new stream controller
      final streamController = StreamController<Map<String, dynamic>>.broadcast();
      _dataStreams[streamKey] = streamController;
      
      // Setup real-time subscription
      final channel = _client
          .channel(streamKey)
          .onPostgresChanges(
            event: PostgresChangeEvent.all,
            schema: 'public',
            table: table,
            filter: filters != null ? PostgresChangeFilter(
              type: PostgresChangeFilterType.eq,
              column: 'workspace_id',
              value: _currentWorkspaceId,
            ) : null,
            callback: (payload) async {
              try {
                var data = payload.newRecord;
                
                // Enhance data with predictions if enabled
                if (enablePredictions && table == 'analytics_events') {
                  final predictions = await _generateEventPredictions(data);
                  data['predictions'] = predictions;
                }
                
                // Add to stream
                streamController.add(data);
                
                // Clear related cache
                _clearCachePattern(table);
                
                // Track real-time event
                await _trackSystemMetric('realtime_event_processed', 1);
              } catch (e) {
                debugPrint('Real-time data processing error: $e');
              }
            },
          );

      await channel.subscribe();
      _realtimeChannels[streamKey] = channel;
      
      debugPrint('üì° Enhanced real-time subscription created for $table');
      return streamController.stream;
    } catch (e) {
      debugPrint('Enhanced real-time subscription failed: $e');
      rethrow;
    }
  }

  /// Generate predictions for events
  Future<Map<String, dynamic>> _generateEventPredictions(Map<String, dynamic> eventData) async {
    try {
      // Simple prediction based on historical patterns
      final eventName = eventData['event_name'] as String?;
      if (eventName == null) return {};
      
      final historicalEvents = await _client
          .from('analytics_events')
          .select('event_data, created_at')
          .eq('workspace_id', _currentWorkspaceId ?? '')
          .eq('event_name', eventName)
          .gte('created_at', DateTime.now().subtract(const Duration(days: 30)).toIso8601String())
          .order('created_at', ascending: false)
          .limit(100);
      
      if (historicalEvents.isEmpty) return {};
      
      // Calculate patterns
      final events = List<Map<String, dynamic>>.from(historicalEvents);
      final avgFrequency = events.length / 30; // events per day
      
      return {
        'next_occurrence_prediction': DateTime.now().add(Duration(days: (1 / avgFrequency).round())).toIso8601String(),
        'frequency_score': avgFrequency,
        'confidence': events.length > 10 ? 0.8 : 0.4,
        'pattern_type': avgFrequency > 5 ? 'high_frequency' : avgFrequency > 1 ? 'moderate' : 'low_frequency',
      };
    } catch (e) {
      debugPrint('Event prediction error: $e');
      return {};
    }
  }

  /// Enhanced bulk operations with transaction support and rollback
  Future<Map<String, dynamic>> performEnhancedBulkOperation({
    required String operation,
    required String table,
    required List<Map<String, dynamic>> data,
    bool enableRollback = true,
    int batchSize = 100,
  }) async {
    try {
      await _ensureInitialized();
      
      final stopwatch = Stopwatch()..start();
      var processedCount = 0;
      var errorCount = 0;
      final errors = <String>[];
      
      // Process in batches
      for (int i = 0; i < data.length; i += batchSize) {
        final batch = data.sublist(i, i + batchSize > data.length ? data.length : i + batchSize);
        
        try {
          switch (operation.toLowerCase()) {
            case 'insert':
              await _client.from(table).insert(batch);
              break;
            case 'upsert':
              await _client.from(table).upsert(batch);
              break;
            case 'update':
              for (final item in batch) {
                if (item['id'] != null) {
                  await _client.from(table).update(item).eq('id', item['id']);
                }
              }
              break;
            case 'delete':
              final ids = batch.map((item) => item['id']).where((id) => id != null).toList();
              if (ids.isNotEmpty) {
                await _client.from(table).delete().inFilter('id', ids);
              }
              break;
          }
          
          processedCount += batch.length;
        } catch (e) {
          errorCount += batch.length;
          errors.add('Batch ${i ~/ batchSize + 1}: $e');
          
          if (enableRollback && errorCount > data.length * 0.1) {
            // If more than 10% errors, consider rollback
            throw Exception('Bulk operation failed with high error rate: ${errorCount / data.length * 100}%');
          }
        }
      }
      
      stopwatch.stop();
      
      // Clear related cache
      _clearCachePattern(table);
      
      // Track operation
      await _trackSystemMetric('bulk_operation_completed', 1);
      await _trackSystemMetric('bulk_operation_duration_ms', stopwatch.elapsedMilliseconds.toDouble());
      
      final result = {
        'operation': operation,
        'table': table,
        'total_records': data.length,
        'processed_records': processedCount,
        'error_records': errorCount,
        'success_rate': (processedCount / data.length * 100).round(),
        'duration_ms': stopwatch.elapsedMilliseconds,
        'errors': errors,
        'status': errorCount == 0 ? 'success' : errorCount < data.length ? 'partial_success' : 'failed',
      };
      
      return result;
    } catch (e) {
      ErrorHandler.handleError('Enhanced bulk operation failed: $e');
      return {
        'operation': operation,
        'table': table,
        'status': 'failed',
        'error': e.toString(),
      };
    }
  }

  /// Intelligent search with caching and performance optimization
  Future<List<Map<String, dynamic>>> performIntelligentSearch({
    required String query,
    List<String>? tables,
    Map<String, List<String>>? searchFields,
    Map<String, dynamic>? filters,
    int? limit,
    bool enableFuzzySearch = true,
    bool enableRanking = true,
  }) async {
    try {
      await _ensureInitialized();
      
      final cacheKey = 'intelligent_search_${query.hashCode}_${tables?.join(',')}_${filters?.hashCode ?? 0}';
      
      return await _getOptimizedData<List<Map<String, dynamic>>>(
        cacheKey,
        () async {
          final allResults = <Map<String, dynamic>>[];
          final searchTables = tables ?? ['social_media_posts', 'crm_contacts', 'content_templates', 'products'];
          
          for (final table in searchTables) {
            try {
              final tableFields = searchFields?[table] ?? ['name', 'title', 'description', 'content'];
              final searchConditions = <String>[];
              
              for (final field in tableFields) {
                if (enableFuzzySearch) {
                  // Use PostgreSQL similarity search
                  searchConditions.add('$field.ilike.%$query%');
                } else {
                  searchConditions.add('$field.eq.$query');
                }
              }
              
              var dbQuery = _client.from(table).select('*, \'$table\' as _source_table');
              
              if (_currentWorkspaceId != null) {
                dbQuery = dbQuery.eq('workspace_id', _currentWorkspaceId as Object);
              }
              
              // Apply search conditions
              if (searchConditions.isNotEmpty) {
                dbQuery = dbQuery.or(searchConditions.join(','));
              }
              
              // Apply filters
              if (filters != null) {
                filters.forEach((key, value) {
                  dbQuery = dbQuery.eq(key, value);
                });
              }
              
              final results = await dbQuery
                  .order('created_at', ascending: false)
                  .limit(limit ?? 20);
              
              final tableResults = List<Map<String, dynamic>>.from(results);
              
              // Add relevance scoring if enabled
              if (enableRanking) {
                for (final result in tableResults) {
                  result['_relevance_score'] = _calculateRelevanceScore(result, query, tableFields);
                }
              }
              
              allResults.addAll(tableResults);
            } catch (e) {
              debugPrint('Search error for table $table: $e');
            }
          }
          
          // Sort by relevance if ranking is enabled
          if (enableRanking) {
            allResults.sort((a, b) => 
              (b['_relevance_score'] as double).compareTo(a['_relevance_score'] as double));
          }
          
          // Track search
          await _trackSystemMetric('intelligent_search_performed', 1);
          await _trackSystemMetric('search_results_count', allResults.length.toDouble());
          
          return allResults.take(limit ?? 50).toList();
        },
        ttl: const Duration(minutes: 5),
      ) ?? [];
    } catch (e) {
      ErrorHandler.handleError('Intelligent search failed: $e');
      return [];
    }
  }

  /// Calculate relevance score for search results
  double _calculateRelevanceScore(Map<String, dynamic> result, String query, List<String> searchFields) {
    double score = 0.0;
    final queryLower = query.toLowerCase();
    
    for (final field in searchFields) {
      final fieldValue = result[field]?.toString().toLowerCase() ?? '';
      
      if (fieldValue.contains(queryLower)) {
        // Exact match gets highest score
        if (fieldValue == queryLower) {
          score += 10.0;
        }
        // Starts with query gets high score
        else if (fieldValue.startsWith(queryLower)) {
          score += 7.0;
        }
        // Contains query gets medium score
        else {
          score += 3.0;
        }
        
        // Boost score based on field importance
        if (field == 'name' || field == 'title') {
          score *= 1.5;
        }
      }
    }
    
    return score;
  }

  /// Advanced file upload with progress tracking and optimization
  Future<Map<String, dynamic>> uploadFileWithAdvancedFeatures({
    required String bucketName,
    required String fileName,
    required List<int> fileBytes,
    String? mimeType,
    Map<String, String>? metadata,
    bool enableCompression = true,
    bool enableDeduplication = true,
    Function(double)? onProgress,
  }) async {
    try {
      await _ensureInitialized();
      
      final userId = _client.auth.currentUser?.id;
      if (userId == null || _currentWorkspaceId == null) {
        throw Exception('User or workspace not available');
      }

      final stopwatch = Stopwatch()..start();
      
      // Generate file hash for deduplication
      final fileHash = _generateAdvancedFileHash(fileBytes);
      
      // Check for existing file if deduplication is enabled
      if (enableDeduplication) {
        final existingFile = await _checkForExistingFile(fileHash);
        if (existingFile != null) {
          await _trackSystemMetric('file_deduplication_hit', 1);
          return {
            'status': 'success',
            'url': existingFile['public_url'],
            'file_id': existingFile['id'],
            'deduplication': true,
            'message': 'File already exists',
          };
        }
      }
      
      // Compress file if enabled and beneficial
      List<int> finalBytes = fileBytes;
      if (enableCompression && _shouldCompress(mimeType, fileBytes.length)) {
        finalBytes = await _compressFile(fileBytes, mimeType);
        await _trackSystemMetric('file_compression_applied', 1);
      }
      
      final filePath = '${_currentWorkspaceId}/${userId}/${DateTime.now().millisecondsSinceEpoch}_$fileName';
      
      // Upload with progress tracking
      onProgress?.call(0.1);
      
      await _client.storage.from(bucketName).uploadBinary(
        filePath,
        Uint8List.fromList(finalBytes),
        fileOptions: FileOptions(
          contentType: mimeType,
          metadata: {
            ...metadata ?? {},
            'original_size': fileBytes.length.toString(),
            'compressed_size': finalBytes.length.toString(),
            'file_hash': fileHash,
          },
        ),
      );
      
      onProgress?.call(0.8);
      
      // Get public URL
      final publicUrl = _client.storage.from(bucketName).getPublicUrl(filePath);
      
      // Store enhanced file metadata
      final fileRecord = await _client.from('file_storage').insert({
        'workspace_id': _currentWorkspaceId,
        'user_id': userId,
        'file_name': fileName,
        'file_path': filePath,
        'file_size': fileBytes.length,
        'compressed_size': finalBytes.length,
        'mime_type': mimeType ?? 'application/octet-stream',
        'file_hash': fileHash,
        'bucket_name': bucketName,
        'is_public': true,
        'compression_ratio': finalBytes.length / fileBytes.length,
        'metadata': {
          ...metadata ?? {},
          'upload_duration_ms': stopwatch.elapsedMilliseconds,
        },
      }).select().single();
      
      onProgress?.call(1.0);
      stopwatch.stop();
      
      // Track upload metrics
      await _trackSystemMetric('file_upload_completed', 1);
      await _trackSystemMetric('file_upload_duration_ms', stopwatch.elapsedMilliseconds.toDouble());
      await _trackSystemMetric('file_upload_size_bytes', fileBytes.length.toDouble());
      
      return {
        'status': 'success',
        'url': publicUrl,
        'file_id': fileRecord['id'],
        'file_size': fileBytes.length,
        'compressed_size': finalBytes.length,
        'compression_ratio': finalBytes.length / fileBytes.length,
        'upload_duration_ms': stopwatch.elapsedMilliseconds,
        'deduplication': false,
      };
    } catch (e) {
      await _trackSystemMetric('file_upload_error', 1);
      ErrorHandler.handleError('Advanced file upload failed: $e');
      return {
        'status': 'error',
        'error': e.toString(),
      };
    }
  }

  /// Check for existing file by hash
  Future<Map<String, dynamic>?> _checkForExistingFile(String fileHash) async {
    try {
      final response = await _client
          .from('file_storage')
          .select('id, file_path, bucket_name')
          .eq('workspace_id', _currentWorkspaceId ?? '')
          .eq('file_hash', fileHash)
          .limit(1);
      
      if (response.isNotEmpty) {
        final existing = response.first;
        final publicUrl = _client.storage
            .from(existing['bucket_name'])
            .getPublicUrl(existing['file_path']);
        
        return {
          'id': existing['id'],
          'public_url': publicUrl,
        };
      }
      
      return null;
    } catch (e) {
      debugPrint('File deduplication check error: $e');
      return null;
    }
  }

  /// Generate advanced file hash
  String _generateAdvancedFileHash(List<int> fileBytes) {
    // Use a more sophisticated hash including size and content sample
    final sizeHash = fileBytes.length.hashCode;
    final contentHash = fileBytes.take(1024).toList().hashCode; // First 1KB
    final endHash = fileBytes.length > 1024 
        ? fileBytes.skip(fileBytes.length - 1024).toList().hashCode 
        : contentHash;
    
    return '${sizeHash}_${contentHash}_$endHash';
  }

  /// Check if file should be compressed
  bool _shouldCompress(String? mimeType, int fileSize) {
    if (fileSize < 1024) return false; // Don't compress small files
    
    final compressibleTypes = [
      'text/',
      'application/json',
      'application/xml',
      'application/javascript',
      'application/css',
    ];
    
    return mimeType != null && compressibleTypes.any((type) => mimeType.startsWith(type));
  }

  /// Compress file (placeholder - implement actual compression)
  Future<List<int>> _compressFile(List<int> fileBytes, String? mimeType) async {
    // This is a placeholder - implement actual compression based on file type
    // For text files, you might use gzip compression
    // For images, you might use image compression libraries
    
    await Future.delayed(const Duration(milliseconds: 100)); // Simulate compression time
    return fileBytes; // Return original for now
  }

  /// Performance monitoring and metrics collection
  void _collectAdvancedMetrics() {
    try {
      // Collect L1 cache metrics
      final cacheHitRate = _calculateCacheHitRate();
      _recordMetric('l1_cache_hit_rate', cacheHitRate);
      _recordMetric('l1_cache_size', _l1Cache.length.toDouble());
      
      // Collect real-time connection metrics
      _recordMetric('realtime_connections', _realtimeChannels.length.toDouble());
      
      // Collect pending operations metrics
      _recordMetric('pending_operations', _pendingOperations.length.toDouble());
      
      // Memory usage estimation
      final estimatedMemoryMB = (_l1Cache.length * 1024) / (1024 * 1024); // Rough estimate
      _recordMetric('estimated_memory_usage_mb', estimatedMemoryMB);
      
    } catch (e) {
      debugPrint('Metrics collection error: $e');
    }
  }

  /// Calculate cache hit rate
  double _calculateCacheHitRate() {
    if (_cacheHitCounts.isEmpty) return 0.0;
    
    final totalAccesses = _cacheHitCounts.values.fold(0, (sum, count) => sum + count);
    final cacheHits = _cacheHitCounts.values.where((count) => count > 0).length;
    
    return totalAccesses > 0 ? (cacheHits / _cacheHitCounts.length) * 100 : 0.0;
  }

  /// Record performance metric
  void _recordMetric(String metricName, double value) {
    _performanceMetrics.putIfAbsent(metricName, () => []).add(value);
    
    // Keep only last 100 measurements
    final metrics = _performanceMetrics[metricName]!;
    if (metrics.length > 100) {
      _performanceMetrics[metricName] = metrics.sublist(metrics.length - 100);
    }
  }

  /// Perform comprehensive health check
  Future<void> _performComprehensiveHealthCheck() async {
    try {
      final healthReport = await _client.rpc('comprehensive_system_health_check');
      
      if (healthReport != null) {
        final healthScore = healthReport['overall_health_score'] as int;
        await _trackSystemMetric('system_health_score', healthScore.toDouble());
        
        if (healthScore < 60) {
          debugPrint('‚ö†Ô∏è System health degraded: Score $healthScore');
          // Could trigger alerts or automated recovery actions
        }
      }
    } catch (e) {
      debugPrint('Health check error: $e');
      await _trackSystemMetric('health_check_error', 1);
    }
  }

  /// Optimize cache strategy
  Future<void> _optimizeCacheStrategy() async {
    try {
      // Clean expired entries
      final now = DateTime.now();
      final expiredKeys = _cacheExpiry.entries
          .where((entry) => now.isAfter(entry.value))
          .map((entry) => entry.key)
          .toList();
      
      for (final key in expiredKeys) {
        _l1Cache.remove(key);
        _cacheExpiry.remove(key);
        _cacheHitCounts.remove(key);
      }
      
      // Analyze cache performance
      final optimization = await _client.rpc('optimize_cache_strategy', params: {
        'workspace_uuid': _currentWorkspaceId,
      });
      
      if (optimization != null) {
        final hitRatio = optimization['overall_hit_ratio'] as double;
        await _trackSystemMetric('cache_optimization_hit_ratio', hitRatio);
        
        if (hitRatio < 60) {
          debugPrint('üìä Cache performance low: ${hitRatio.toStringAsFixed(1)}%');
          // Could implement cache strategy adjustments
        }
      }
    } catch (e) {
      debugPrint('Cache optimization error: $e');
    }
  }

  /// Check and restore real-time connections
  void _checkAndRestoreConnections() {
    _realtimeChannels.forEach((key, channel) {
      if (channel.socket.isConnected == false) {
        debugPrint('üîÑ Restoring real-time connection: $key');
        channel.subscribe();
      }
    });
  }

  /// Clear cache by pattern
  void _clearCachePattern(String pattern) {
    final keysToRemove = _l1Cache.keys
        .where((key) => key.contains(pattern))
        .toList();
    
    for (final key in keysToRemove) {
      _l1Cache.remove(key);
      _cacheExpiry.remove(key);
      _cacheHitCounts.remove(key);
    }
  }

  /// Track system metric
  Future<void> _trackSystemMetric(String metricName, double value) async {
    try {
      await _client.rpc('record_health_metric', params: {
        'metric_name_param': metricName,
        'metric_value_param': value,
        'metric_unit_param': _getMetricUnit(metricName),
        'tags_param': {
          'service': 'enhanced_data_service',
          'workspace_id': _currentWorkspaceId,
          'platform': Platform.operatingSystem,
        },
      });
    } catch (e) {
      // Silently fail for metrics to avoid affecting app performance
    }
  }

  /// Track cache performance
  Future<void> _trackCachePerformance(String cacheKey, bool isHit, int durationMs) async {
    try {
      await _client.from('intelligent_cache_analytics').upsert({
        'workspace_id': _currentWorkspaceId,
        'cache_key': cacheKey,
        'cache_layer': 'memory',
        'hit_count': isHit ? 1 : 0,
        'miss_count': isHit ? 0 : 1,
        'last_accessed': DateTime.now().toIso8601String(),
        'access_pattern': {
          'avg_duration_ms': durationMs,
          'last_access_type': isHit ? 'hit' : 'miss',
        },
        'updated_at': DateTime.now().toIso8601String(),
      }, onConflict: 'workspace_id,cache_key');
    } catch (e) {
      // Silently fail for cache analytics
    }
  }

  /// Get metric unit
  String _getMetricUnit(String metricName) {
    if (metricName.contains('_ms')) return 'milliseconds';
    if (metricName.contains('_bytes')) return 'bytes';
    if (metricName.contains('_mb')) return 'megabytes';
    if (metricName.contains('_rate') || metricName.contains('_ratio')) return 'percentage';
    if (metricName.contains('_count') || metricName.contains('_size')) return 'count';
    if (metricName.contains('_score')) return 'score';
    return 'count';
  }

  /// Get service status
  Map<String, dynamic> getEnhancedServiceStatus() {
    return {
      'is_initialized': _isInitialized,
      'current_workspace_id': _currentWorkspaceId,
      'l1_cache_size': _l1Cache.length,
      'cache_hit_rate': _calculateCacheHitRate(),
      'realtime_connections': _realtimeChannels.length,
      'data_streams': _dataStreams.length,
      'pending_operations': _pendingOperations.length,
      'is_syncing': _isSyncing,
      'performance_metrics': _performanceMetrics.map(
        (key, value) => MapEntry(key, value.isNotEmpty ? value.last : 0.0),
      ),
      'monitoring_active': _performanceMonitorTimer?.isActive ?? false,
    };
  }

  /// Dispose and cleanup
  Future<void> dispose() async {
    // Cancel timers
    _performanceMonitorTimer?.cancel();
    _cacheOptimizationTimer?.cancel();
    _healthCheckTimer?.cancel();
    
    // Unsubscribe from real-time channels
    for (final channel in _realtimeChannels.values) {
      await channel.unsubscribe();
    }
    _realtimeChannels.clear();
    
    // Close data streams
    for (final controller in _dataStreams.values) {
      await controller.close();
    }
    _dataStreams.clear();
    
    // Clear caches
    _l1Cache.clear();
    _cacheExpiry.clear();
    _cacheHitCounts.clear();
    _performanceMetrics.clear();
    _pendingOperations.clear();
    
    debugPrint('üßπ Enhanced Production Data Service disposed');
  }
}